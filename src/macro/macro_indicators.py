#!/usr/bin/env python3
"""
============================================================
å®è§‚æŒ‡æ ‡æ•°æ®æ¨¡å—
============================================================

ç®¡ç†é•/ä¸é”ˆé’¢ç›¸å…³çš„å®è§‚ç»æµæŒ‡æ ‡ï¼š
- EDBç»æµæ•°æ®åº“æŒ‡æ ‡ï¼ˆç¾å…ƒæŒ‡æ•°ã€LMEé•åº“å­˜ç­‰ï¼‰
- æœŸè´§è¡Œæƒ…æŒ‡æ ‡ï¼ˆæ²ªé•è¿ç»­ç­‰ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    from src.macro.macro_indicators import MacroDataFetcher
    fetcher = MacroDataFetcher(refresh_token)
    data = fetcher.fetch_all()
============================================================
"""

import os
import sys
import csv
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.data_fetcher.ths_client import TonghuashunClient


# ============================================================
# å®è§‚æŒ‡æ ‡é…ç½®
# ============================================================

# EDBç»æµæ•°æ®åº“æŒ‡æ ‡ï¼ˆé€šè¿‡ edb_service æ¥å£æŸ¥è¯¢ï¼‰
EDB_INDICATORS = {
    "LMEé•åº“å­˜": {
        "id": "S004303610",
        "unit": "å¨",
        "frequency": "æ—¥",
        "category": "åº“å­˜",
        "description": "LMEé•åº“å­˜é‡ï¼Œåæ˜ å…¨çƒé•ä¾›éœ€çŠ¶å†µçš„æ ¸å¿ƒæŒ‡æ ‡"
    },
    "ç¾å…ƒæŒ‡æ•°": {
        "id": "G002600885",
        "unit": "ç‚¹",
        "frequency": "æ—¥",
        "category": "å®è§‚",
        "description": "ç¾å…ƒå¯¹ä¸€ç¯®å­è´§å¸çš„æ±‡ç‡æŒ‡æ•°ï¼Œå½±å“ä»¥ç¾å…ƒè®¡ä»·çš„å•†å“ä»·æ ¼"
    },
}

# æœŸè´§è¡Œæƒ…æŒ‡æ ‡ï¼ˆé€šè¿‡ cmd_history_quotation æ¥å£æŸ¥è¯¢ï¼‰
FUTURES_INDICATORS = {
    "æ²ªé•è¿ç»­": {
        "code": "NI00.SHF",
        "unit": "å…ƒ/å¨",
        "frequency": "æ—¥",
        "category": "ä»·æ ¼",
        "description": "ä¸ŠæœŸæ‰€é•æœŸè´§è¿ç»­åˆçº¦ï¼Œå›½å†…é•å®šä»·åŸºå‡†"
    },
}

# å¾…è¡¥å……çš„æŒ‡æ ‡ï¼ˆæŸ¥åˆ°IDåæ·»åŠ ï¼‰
PENDING_INDICATORS = """
ä»¥ä¸‹æŒ‡æ ‡å¾…æŸ¥è¯¢IDåè¡¥å……ï¼š
- ä¸ŠæœŸæ‰€é•åº“å­˜
- ä¸­å›½PMI
- æ²ªé•ä»“å•
- é•çŸ¿è¿›å£é‡
- ä¸é”ˆé’¢äº§é‡
- å°å°¼é•çŸ¿å‡ºå£æ”¿ç­–ç›¸å…³
"""


class MacroDataFetcher:
    """å®è§‚æ•°æ®æ‹‰å–å™¨"""

    def __init__(self, refresh_token: str):
        self.client = TonghuashunClient(refresh_token)
        self.data_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
            "data", "macro"
        )
        os.makedirs(self.data_dir, exist_ok=True)

    def fetch_edb_indicator(self, name: str, days: int = 365) -> Optional[List[dict]]:
        """
        æ‹‰å–å•ä¸ªEDBæŒ‡æ ‡æ•°æ®

        å‚æ•°:
            name: æŒ‡æ ‡åç§°ï¼ˆå¦‚"LMEé•åº“å­˜"ï¼‰
            days: æ‹‰å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®

        è¿”å›:
            [{"date": "2025-01-01", "value": 12345.0}, ...]
        """
        if name not in EDB_INDICATORS:
            print(f"âŒ æœªçŸ¥æŒ‡æ ‡: {name}")
            return None

        config = EDB_INDICATORS[name]
        indicator_id = config["id"]

        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        print(f"ğŸ“¡ æ‹‰å– {name} ({indicator_id}): {start_date} ~ {end_date}")

        try:
            result = self.client.get_edb_data(
                indicator_ids=indicator_id,
                start_date=start_date,
                end_date=end_date
            )

            # è§£æè¿”å›æ•°æ®
            if 'tables' in result and len(result['tables']) > 0:
                table = result['tables'][0]
                time_list = table.get('time', [])
                data_table = table.get('table', {})

                # EDBè¿”å›çš„æ•°æ®ç»“æ„ï¼štable é‡Œçš„ key æ˜¯æŒ‡æ ‡ID
                values = data_table.get(indicator_id, [])

                rows = []
                for i, date in enumerate(time_list):
                    val = values[i] if i < len(values) else None
                    if val is not None and val != '':
                        try:
                            rows.append({
                                "date": date,
                                "value": float(val) if val else None
                            })
                        except (ValueError, TypeError):
                            pass

                print(f"  âœ… è·å– {len(rows)} æ¡æ•°æ®")
                return rows
            else:
                print(f"  âš ï¸ è¿”å›æ•°æ®ä¸ºç©º")
                if 'errmsg' in result:
                    print(f"  é”™è¯¯ä¿¡æ¯: {result['errmsg']}")
                return []

        except Exception as e:
            print(f"  âŒ æ‹‰å–å¤±è´¥: {e}")
            return None

    def fetch_futures_indicator(self, name: str, days: int = 365) -> Optional[List[dict]]:
        """
        æ‹‰å–æœŸè´§è¡Œæƒ…æŒ‡æ ‡

        å‚æ•°:
            name: æŒ‡æ ‡åç§°ï¼ˆå¦‚"æ²ªé•è¿ç»­"ï¼‰
            days: æ‹‰å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®

        è¿”å›:
            [{"date": "2025-01-01", "open": 123, "high": 125, "low": 121, "close": 124, "volume": 1000}, ...]
        """
        if name not in FUTURES_INDICATORS:
            print(f"âŒ æœªçŸ¥æŒ‡æ ‡: {name}")
            return None

        config = FUTURES_INDICATORS[name]
        code = config["code"]

        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")

        print(f"ğŸ“¡ æ‹‰å– {name} ({code}): {start_date} ~ {end_date}")

        try:
            result = self.client.get_history_quotes(
                codes=code,
                indicators="open,high,low,close,volume,amount,openInterest",
                start_date=start_date,
                end_date=end_date
            )

            if 'tables' in result and len(result['tables']) > 0:
                table = result['tables'][0]
                time_list = table.get('time', [])
                data_table = table.get('table', {})

                rows = []
                for i, date in enumerate(time_list):
                    row = {"date": date}
                    for key in ['open', 'high', 'low', 'close', 'volume', 'amount', 'openInterest']:
                        vals = data_table.get(key, [])
                        if i < len(vals) and vals[i] is not None and vals[i] != '':
                            try:
                                row[key] = float(vals[i])
                            except (ValueError, TypeError):
                                row[key] = None
                        else:
                            row[key] = None
                    rows.append(row)

                print(f"  âœ… è·å– {len(rows)} æ¡æ•°æ®")
                return rows
            else:
                print(f"  âš ï¸ è¿”å›æ•°æ®ä¸ºç©º")
                return []

        except Exception as e:
            print(f"  âŒ æ‹‰å–å¤±è´¥: {e}")
            return None

    def fetch_all(self, days: int = 365) -> Dict[str, List[dict]]:
        """
        æ‹‰å–æ‰€æœ‰å·²é…ç½®çš„å®è§‚æŒ‡æ ‡

        è¿”å›:
            {
                "LMEé•åº“å­˜": [{"date": ..., "value": ...}, ...],
                "ç¾å…ƒæŒ‡æ•°": [...],
                "æ²ªé•è¿ç»­": [{"date": ..., "open": ..., "close": ...}, ...],
            }
        """
        all_data = {}

        print("\n" + "=" * 60)
        print("ğŸ“Š æ‹‰å–å®è§‚æŒ‡æ ‡æ•°æ®")
        print("=" * 60)

        # æ‹‰å– EDB æŒ‡æ ‡
        for name in EDB_INDICATORS:
            data = self.fetch_edb_indicator(name, days)
            if data:
                all_data[name] = data

        # æ‹‰å–æœŸè´§æŒ‡æ ‡
        for name in FUTURES_INDICATORS:
            data = self.fetch_futures_indicator(name, days)
            if data:
                all_data[name] = data

        return all_data

    def save_to_csv(self, all_data: Dict[str, List[dict]]):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        for name, rows in all_data.items():
            if not rows:
                continue

            safe_name = name.replace("/", "_").replace(" ", "_")
            filepath = os.path.join(self.data_dir, f"{safe_name}.csv")

            fieldnames = list(rows[0].keys())
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(rows)

            print(f"  ğŸ’¾ {filepath} ({len(rows)} æ¡)")

    def load_from_csv(self) -> Dict[str, List[dict]]:
        """ä»CSVåŠ è½½å·²æœ‰æ•°æ®"""
        all_data = {}

        if not os.path.exists(self.data_dir):
            return all_data

        for filename in os.listdir(self.data_dir):
            if not filename.endswith('.csv'):
                continue

            name = filename.replace('.csv', '').replace('_', ' ')
            filepath = os.path.join(self.data_dir, filename)

            rows = []
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # è½¬æ¢æ•°å€¼ç±»å‹
                    for key in row:
                        if key != 'date' and row[key]:
                            try:
                                row[key] = float(row[key])
                            except ValueError:
                                pass
                    rows.append(row)

            all_data[name] = rows

        return all_data


# ============================================================
# å‘½ä»¤è¡Œæµ‹è¯•
# ============================================================

def main():
    """æµ‹è¯•å®è§‚æ•°æ®æ‹‰å–"""
    import sys
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from config.settings import IFIND_REFRESH_TOKEN

    if not IFIND_REFRESH_TOKEN or IFIND_REFRESH_TOKEN == "åœ¨è¿™é‡Œå¡«å†™ä½ çš„refresh_token":
        print("âŒ è¯·å…ˆåœ¨ config/settings.py é…ç½® refresh_token")
        return

    fetcher = MacroDataFetcher(IFIND_REFRESH_TOKEN)

    # æ‹‰å–æœ€è¿‘1å¹´çš„æ•°æ®
    all_data = fetcher.fetch_all(days=365)

    # ä¿å­˜åˆ°CSV
    print("\nğŸ“ ä¿å­˜æ•°æ®åˆ° data/macro/ ...")
    fetcher.save_to_csv(all_data)

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ•°æ®æ‘˜è¦")
    print("=" * 60)

    for name, rows in all_data.items():
        if not rows:
            continue

        latest = rows[-1]
        print(f"\nã€{name}ã€‘")
        print(f"  æœ€æ–°æ—¥æœŸ: {latest.get('date', 'N/A')}")

        if 'value' in latest:
            print(f"  æœ€æ–°å€¼: {latest['value']}")
        elif 'close' in latest:
            print(f"  æ”¶ç›˜ä»·: {latest.get('close', 'N/A')}")
            print(f"  æŒä»“é‡: {latest.get('openInterest', 'N/A')}")

        # è®¡ç®—å˜åŒ–
        if len(rows) >= 2:
            prev = rows[-2]
            if 'value' in latest and 'value' in prev:
                if prev['value'] and latest['value']:
                    chg = latest['value'] - prev['value']
                    pct = (chg / prev['value']) * 100
                    print(f"  æ—¥å˜åŒ–: {chg:+.2f} ({pct:+.2f}%)")
            elif 'close' in latest and 'close' in prev:
                if prev['close'] and latest['close']:
                    chg = latest['close'] - prev['close']
                    pct = (chg / prev['close']) * 100
                    print(f"  æ—¥æ¶¨è·Œ: {chg:+.0f} ({pct:+.2f}%)")


if __name__ == "__main__":
    main()
